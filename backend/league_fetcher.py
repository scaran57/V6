# /app/backend/league_fetcher.py
"""
R√©cup√©ration automatique des classements de ligues depuis Wikipedia.
Syst√®me robuste avec parsing standard des tableaux Wikipedia.
"""
import requests
from bs4 import BeautifulSoup
import json
import os
import re
import unicodedata
from datetime import datetime, timezone
import logging

logger = logging.getLogger(__name__)

# Configuration TTL (Time To Live) pour le cache
DEFAULT_TTL = 24 * 3600  # 24 heures en secondes

DATA_DIR = "/app/data/leagues"
os.makedirs(DATA_DIR, exist_ok=True)

# Configuration des ligues avec leurs URLs et m√©thodes de parsing
LEAGUE_CONFIG = {
    "LaLiga": {
        "url": "https://en.wikipedia.org/wiki/2025‚Äì26_La_Liga",
        "method": "scrape_laliga",
        "fallback_file": f"{DATA_DIR}/LaLiga.json"
    },
    "PremierLeague": {
        "url": "https://en.wikipedia.org/wiki/2025‚Äì26_Premier_League",
        "method": "scrape_premier_league",
        "fallback_file": f"{DATA_DIR}/PremierLeague.json"
    },
    "SerieA": {
        "url": "https://en.wikipedia.org/wiki/2025‚Äì26_Serie_A",
        "method": "scrape_serie_a",
        "fallback_file": f"{DATA_DIR}/SerieA.json"
    },
    "Ligue1": {
        "url": "https://en.wikipedia.org/wiki/2025‚Äì26_Ligue_1",
        "method": "scrape_ligue1",
        "fallback_file": f"{DATA_DIR}/Ligue1.json"
    },
    "Bundesliga": {
        "url": "https://en.wikipedia.org/wiki/2025‚Äì26_Bundesliga",
        "method": "scrape_bundesliga",
        "fallback_file": f"{DATA_DIR}/Bundesliga.json"
    },
    "PrimeiraLiga": {
        "url": "https://en.wikipedia.org/wiki/2025‚Äì26_Primeira_Liga",
        "method": "scrape_primeira_liga",
        "fallback_file": f"{DATA_DIR}/PrimeiraLiga.json"
    },
    "ChampionsLeague": {
        "url": "https://en.wikipedia.org/wiki/2024‚Äì25_UEFA_Champions_League",
        "method": "scrape_champions_league",
        "fallback_file": f"{DATA_DIR}/ChampionsLeague.json"
    },
    "EuropaLeague": {
        "url": "https://en.wikipedia.org/wiki/2024‚Äì25_UEFA_Europa_League",
        "method": "scrape_europa_league",
        "fallback_file": f"{DATA_DIR}/EuropaLeague.json"
    }
}

# Alias pour compatibilit√© avec server.py
WIKI_MAP = LEAGUE_CONFIG

def _save_json(path, obj):
    """Sauvegarde atomique JSON"""
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, ensure_ascii=False)
        f.flush()
        os.fsync(f.fileno())
    os.replace(tmp, path)

def _load_json(path):
    """Charge JSON avec gestion d'erreur"""
    if not os.path.exists(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {}

def _normalize_name(name):
    """Normalise le nom d'√©quipe pour matching"""
    name = re.sub(r"\(.*?\)", "", name)
    name = name.strip()
    name = unicodedata.normalize("NFKD", name)
    name = "".join([c for c in name if not unicodedata.combining(c)])
    name = re.sub(r"\s+", " ", name)
    return name.strip()

def scrape_laliga(html):
    """Parse le classement LaLiga officiel"""
    try:
        soup = BeautifulSoup(html, "lxml")
        teams = []
        
        # LaLiga utilise une structure avec des divs/tables sp√©cifiques
        # Chercher les lignes du classement
        rows = soup.find_all("tr", class_=re.compile("standing.*|table-row.*"))
        
        if not rows:
            # Fallback : chercher toute table avec des donn√©es de classement
            tables = soup.find_all("table")
            for table in tables:
                rows = table.find_all("tr")
                break
        
        for i, row in enumerate(rows, 1):
            cols = row.find_all(["td", "th"])
            if len(cols) < 3:
                continue
            
            # Extraire le nom de l'√©quipe (g√©n√©ralement dans un <a> ou <span>)
            team_name = None
            for col in cols:
                link = col.find("a")
                if link:
                    text = link.get_text(strip=True)
                    if len(text) > 2 and not text.isdigit():
                        team_name = text
                        break
                
                span = col.find("span", class_=re.compile("team.*|club.*"))
                if span:
                    text = span.get_text(strip=True)
                    if len(text) > 2:
                        team_name = text
                        break
            
            if team_name:
                teams.append({
                    "name": _normalize_name(team_name),
                    "rank": i,
                    "points": 0  # Les points peuvent √™tre extraits si besoin
                })
                
                if len(teams) >= 20:  # LaLiga a 20 √©quipes
                    break
        
        return teams
    except Exception as e:
        logger.error(f"Erreur parsing LaLiga: {e}")
        return []

def scrape_premier_league(html):
    """Parse le classement Premier League officiel"""
    try:
        soup = BeautifulSoup(html, "lxml")
        teams = []
        
        # Premier League utilise des classes sp√©cifiques
        rows = soup.find_all("tr", class_=re.compile("table__row.*"))
        
        if not rows:
            # Fallback
            tables = soup.find_all("table")
            for table in tables:
                tbody = table.find("tbody")
                if tbody:
                    rows = tbody.find_all("tr")
                    break
        
        for i, row in enumerate(rows, 1):
            # Chercher le nom de l'√©quipe
            team_cell = row.find("td", class_=re.compile("team.*"))
            if not team_cell:
                team_cell = row.find("span", class_=re.compile("team.*|long.*"))
            
            if team_cell:
                team_name = team_cell.get_text(strip=True)
                if len(team_name) > 2:
                    teams.append({
                        "name": _normalize_name(team_name),
                        "rank": i,
                        "points": 0
                    })
                    
                    if len(teams) >= 20:
                        break
        
        return teams
    except Exception as e:
        logger.error(f"Erreur parsing Premier League: {e}")
        return []

def scrape_serie_a(html):
    """Parse Serie A (placeholder - √† impl√©menter phase 2)"""
    logger.warning("Serie A parser pas encore impl√©ment√©")
    return []

def scrape_ligue1(html):
    """Parse Ligue 1 (placeholder - √† impl√©menter phase 2)"""
    logger.warning("Ligue 1 parser pas encore impl√©ment√©")
    return []

def scrape_bundesliga(html):
    """Parse Bundesliga (placeholder - √† impl√©menter phase 2)"""
    logger.warning("Bundesliga parser pas encore impl√©ment√©")
    return []

def scrape_primeira_liga(html):
    """Parse Primeira Liga (placeholder - √† impl√©menter phase 2)"""
    logger.warning("Primeira Liga parser pas encore impl√©ment√©")
    return []

def scrape_champions_league(html):
    """
    Parse Champions League - Extrait les √©quipes participantes.
    Pour la Champions League, on ne peut pas vraiment faire un classement 1-36,
    donc on liste simplement les √©quipes participantes avec un rang fictif bas√©
    sur l'ordre alphab√©tique ou l'ordre d'apparition.
    """
    try:
        soup = BeautifulSoup(html, "lxml")
        teams = []
        seen = set()
        
        # Chercher les liens vers les pages d'√©quipes
        # Les √©quipes de Champions League ont des liens vers leurs pages wiki
        links = soup.find_all("a", href=re.compile(r"/wiki/.*_F\.?C\.?$|/wiki/.*_United|/wiki/.*_City|/wiki/Real_Madrid|/wiki/FC_Barcelona"))
        
        # √âgalement chercher dans les tableaux de groupes
        tables = soup.find_all("table", class_=re.compile("wikitable"))
        for table in tables:
            rows = table.find_all("tr")
            for row in rows:
                team_cell = row.find("td", class_=re.compile("team|club"))
                if not team_cell:
                    team_cell = row.find("a")
                
                if team_cell:
                    team_link = team_cell.find("a") if team_cell.name != "a" else team_cell
                    if team_link:
                        team_name = team_link.get_text(strip=True)
                        if len(team_name) > 2 and team_name not in seen:
                            seen.add(team_name)
                            teams.append({
                                "name": _normalize_name(team_name),
                                "rank": len(teams) + 1,
                                "points": 0
                            })
        
        # Si pas assez d'√©quipes trouv√©es, chercher tous les liens d'√©quipes
        if len(teams) < 10:
            all_links = soup.find_all("a")
            for link in all_links:
                text = link.get_text(strip=True)
                href = link.get("href", "")
                
                # Filtrer les liens d'√©quipes (heuristique)
                if href.startswith("/wiki/") and len(text) > 3 and text not in seen:
                    # √âviter les liens non-√©quipes
                    skip_words = ["UEFA", "Wikipedia", "League", "Group", "Knockout", "Final", "Season", "Match"]
                    if any(word in text for word in skip_words):
                        continue
                    
                    # Ajouter si ressemble √† un nom d'√©quipe
                    if any(word in text for word in ["FC", "CF", "United", "City", "Madrid", "Barcelona", "Milan", "Juventus", "Bayern", "PSG", "Dortmund", "Liverpool", "Arsenal", "Chelsea", "Manchester"]):
                        seen.add(text)
                        teams.append({
                            "name": _normalize_name(text),
                            "rank": len(teams) + 1,
                            "points": 0
                        })
        
        # Limiter √† 36 √©quipes (nouveau format Champions League)
        teams = teams[:36]
        
        logger.info(f"Champions League: {len(teams)} √©quipes extraites")
        return teams
        
    except Exception as e:
        logger.error(f"Erreur parsing Champions League: {e}")
        return []

def scrape_europa_league(html):
    """
    Parse Europa League - Extrait les √©quipes participantes.
    M√™me logique que Champions League mais pour l'Europa League.
    """
    try:
        soup = BeautifulSoup(html, "lxml")
        teams = []
        seen = set()
        
        # Chercher dans les tableaux
        tables = soup.find_all("table", class_=re.compile("wikitable"))
        for table in tables:
            rows = table.find_all("tr")
            for row in rows:
                team_cell = row.find("td", class_=re.compile("team|club"))
                if not team_cell:
                    team_cell = row.find("a")
                
                if team_cell:
                    team_link = team_cell.find("a") if team_cell.name != "a" else team_cell
                    if team_link:
                        team_name = team_link.get_text(strip=True)
                        if len(team_name) > 2 and team_name not in seen:
                            seen.add(team_name)
                            teams.append({
                                "name": _normalize_name(team_name),
                                "rank": len(teams) + 1,
                                "points": 0
                            })
        
        # Fallback : chercher les liens d'√©quipes
        if len(teams) < 10:
            all_links = soup.find_all("a")
            for link in all_links:
                text = link.get_text(strip=True)
                href = link.get("href", "")
                
                if href.startswith("/wiki/") and len(text) > 3 and text not in seen:
                    skip_words = ["UEFA", "Wikipedia", "League", "Group", "Knockout", "Final", "Season", "Match"]
                    if any(word in text for word in skip_words):
                        continue
                    
                    if any(word in text for word in ["FC", "CF", "United", "Roma", "Lazio", "Sevilla", "Villarreal", "Ajax", "Feyenoord", "Porto", "Benfica", "Sporting"]):
                        seen.add(text)
                        teams.append({
                            "name": _normalize_name(text),
                            "rank": len(teams) + 1,
                            "points": 0
                        })
        
        # Limiter √† 36 √©quipes (nouveau format)
        teams = teams[:36]
        
        logger.info(f"Europa League: {len(teams)} √©quipes extraites")
        return teams
        
    except Exception as e:
        logger.error(f"Erreur parsing Europa League: {e}")
        return []

def load_league_data(league_name):
    """Charge les donn√©es d'une ligue depuis son fichier JSON"""
    config = LEAGUE_CONFIG.get(league_name)
    if not config:
        return None
    
    filepath = config["fallback_file"]
    if not os.path.exists(filepath):
        return None
    
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return None

def save_league_data(league_name, data):
    """Sauvegarde les donn√©es d'une ligue dans son fichier JSON"""
    config = LEAGUE_CONFIG.get(league_name)
    if not config:
        return False
    
    filepath = config["fallback_file"]
    
    # Format standardis√©
    output = {
        "league": league_name,
        "updated": datetime.now().isoformat() + "Z",
        "teams": data
    }
    
    try:
        _save_json(filepath, output)
        return True
    except Exception as e:
        logger.error(f"Erreur sauvegarde {league_name}: {e}")
        return False

def load_positions():
    """Charge tous les classements (format compatible ancien syst√®me)"""
    all_standings = {}
    
    for league_name in LEAGUE_CONFIG.keys():
        data = load_league_data(league_name)
        if data and "teams" in data:
            # Convertir au format ancien : {team_name: rank}
            standings = {}
            for team in data["teams"]:
                standings[team["name"]] = team["rank"]
            all_standings[league_name] = standings
    
    return all_standings

def update_league(league_name, force=False):
    """
    Met √† jour le classement d'une ligue depuis son site officiel.
    Utilise le cache si disponible et non expir√© (sauf si force=True).
    """
    config = LEAGUE_CONFIG.get(league_name)
    if not config:
        logger.warning(f"‚ö†Ô∏è Ligue inconnue: {league_name}")
        return {}
    
    # V√©rifier le cache (donn√©es locales)
    cached_data = load_league_data(league_name)
    if cached_data and not force:
        try:
            updated_dt = datetime.fromisoformat(cached_data["updated"].replace("Z", "+00:00"))
            age_seconds = (datetime.now(timezone.utc) - updated_dt).total_seconds()
            
            if age_seconds < DEFAULT_TTL:
                logger.info(f"üìä Classement {league_name} en cache (√¢ge: {int(age_seconds/3600)}h)")
                # Retourner au format ancien pour compatibilit√©
                standings = {team["name"]: team["rank"] for team in cached_data["teams"]}
                return standings
        except Exception as e:
            logger.warning(f"Erreur lecture cache {league_name}: {e}")
    
    # R√©cup√©ration depuis le site officiel
    url = config["url"]
    method_name = config["method"]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    try:
        logger.info(f"üåê R√©cup√©ration classement {league_name} depuis {url}...")
        r = requests.get(url, headers=headers, timeout=20)
        
        if r.status_code == 200:
            # Appeler la fonction de parsing appropri√©e
            parser_func = globals().get(method_name)
            if not parser_func:
                logger.error(f"‚ùå Parser {method_name} introuvable")
                return _fallback_to_cache(league_name, cached_data)
            
            teams = parser_func(r.text)
            
            if not teams:
                logger.warning(f"‚ö†Ô∏è Aucune √©quipe extraite pour {league_name}")
                return _fallback_to_cache(league_name, cached_data)
            
            # Sauvegarder
            save_league_data(league_name, teams)
            
            logger.info(f"‚úÖ Classement {league_name} mis √† jour: {len(teams)} √©quipes")
            
            # Retourner au format ancien pour compatibilit√©
            standings = {team["name"]: team["rank"] for team in teams}
            return standings
        else:
            logger.error(f"‚ùå Erreur HTTP {r.status_code} pour {league_name}")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration de {league_name}: {e}")
    
    # Fallback sur le cache
    return _fallback_to_cache(league_name, cached_data)

def _fallback_to_cache(league_name, cached_data):
    """Utilise les donn√©es en cache si disponibles"""
    if cached_data and "teams" in cached_data:
        logger.warning(f"‚ö†Ô∏è Utilisation du cache pour {league_name}")
        standings = {team["name"]: team["rank"] for team in cached_data["teams"]}
        return standings
    else:
        logger.error(f"‚ùå Aucune donn√©e disponible pour {league_name}")
        return {}

def update_all(leagues=None, force=False):
    """Met √† jour tous les classements (ou une liste sp√©cifique)"""
    if leagues is None:
        leagues = list(LEAGUE_CONFIG.keys())
    
    result = {}
    for lg in leagues:
        try:
            result[lg] = update_league(lg, force=force)
        except Exception as e:
            logger.error(f"‚ùå Erreur update {lg}: {e}")
            result[lg] = {}
    
    return result

def get_team_position(team_name, league_name):
    """
    R√©cup√®re la position d'une √©quipe dans une ligue.
    Fuzzy matching pour g√©rer les variations de noms.
    """
    pos_map = load_positions().get(league_name, {})
    normalized = _normalize_name(team_name)
    
    # Exact match
    if normalized in pos_map:
        return pos_map[normalized]
    
    # Fuzzy match
    for t, p in pos_map.items():
        if normalized == t:
            return p
        # Contient ou est contenu
        if normalized in t or t in normalized:
            return p
    
    return None

if __name__ == "__main__":
    print("üîÑ Mise √† jour de toutes les ligues...")
    r = update_all(force=True)
    for k, v in r.items():
        print(f"  {k}: {len(v)} √©quipes")
    print(f"‚úÖ Sauvegard√© dans {DATA_DIR}")
