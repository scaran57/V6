#!/usr/bin/env python3
# /app/backend/utils/migrate_old_analyses.py
"""
Migration des anciennes analyses vers le cache unifiÃ© UFA.
Fusionne : analyzer_uefa.jsonl, production_cache.jsonl â†’ analysis_cache.jsonl
Ã‰vite les doublons automatiquement.
GÃ©nÃ¨re un rapport statistique dÃ©taillÃ©.
"""

import json
from pathlib import Path
from datetime import datetime
from collections import Counter

BASE = Path("/app/data")
TARGET = BASE / "analysis_cache.jsonl"
REPORT_LOG = Path("/app/logs/migration_report.log")
OLD_FILES = [
    BASE / "analyzer_uefa.jsonl",
    BASE / "production_cache.jsonl",
    BASE / "uefa_analysis_cache.jsonl",
    BASE / "matches_memory.json"  # Ancien cache JSON
]

def read_jsonl(path):
    """Lit un fichier JSONL et retourne une liste d'objets"""
    if not path.exists():
        return []
    out = []
    with path.open(encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                out.append(json.loads(line))
            except Exception as e:
                print(f"âš ï¸  Ligne ignorÃ©e dans {path.name}: {e}")
                continue
    return out

def read_json(path):
    """Lit un fichier JSON et retourne un objet ou liste"""
    if not path.exists():
        return None
    try:
        with path.open(encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸  Erreur lecture {path.name}: {e}")
        return None

def write_jsonl(path, data):
    """Ã‰crit une liste d'objets dans un fichier JSONL"""
    with path.open("w", encoding="utf-8") as f:
        for d in data:
            f.write(json.dumps(d, ensure_ascii=False) + "\n")

def normalize_entry(entry, source_file):
    """Normalise une entrÃ©e d'analyse pour le nouveau format"""
    # Format cible : celui de unified_analyzer
    normalized = {
        "timestamp": entry.get("timestamp", datetime.utcnow().isoformat()),
        "source": entry.get("source", f"migrated_from_{source_file}"),
        "home_team": entry.get("home_team") or entry.get("homeTeam"),
        "away_team": entry.get("away_team") or entry.get("awayTeam"),
        "league": entry.get("league") or entry.get("competition") or "Unknown",
        "home_goals_detected": entry.get("home_goals") or entry.get("home_goals_detected"),
        "away_goals_detected": entry.get("away_goals") or entry.get("away_goals_detected"),
        "raw_text": entry.get("raw_text", ""),
        "prediction": entry.get("prediction", {})
    }
    
    # Si pas de prÃ©diction structurÃ©e, essayer de la construire
    if not normalized["prediction"] or not isinstance(normalized["prediction"], dict):
        normalized["prediction"] = {
            "status": "migrated",
            "most_probable": entry.get("mostProbableScore") or entry.get("predicted_score", "N/A"),
            "probabilities": entry.get("probabilities", {}),
            "confidence": entry.get("confidence", 0.0),
            "league_coeffs_applied": entry.get("leagueCoeffsApplied", False),
            "top3": entry.get("top3", [])
        }
    
    return normalized

def generate_key(entry):
    """GÃ©nÃ¨re une clÃ© unique pour dÃ©tecter les doublons"""
    home = str(entry.get("home_team", "?")).lower().strip()
    away = str(entry.get("away_team", "?")).lower().strip()
    league = str(entry.get("league", "?")).lower().strip()
    timestamp = entry.get("timestamp", "?")
    
    # ClÃ© basÃ©e sur Ã©quipes + ligue + date (ignorer heure pour dÃ©tecter doublons du mÃªme jour)
    date_part = timestamp[:10] if timestamp and len(timestamp) >= 10 else "?"
    return f"{home}-{away}-{league}-{date_part}"

def migrate():
    """Fonction principale de migration"""
    print("=" * 60)
    print("ğŸ”„ MIGRATION DES ANALYSES VERS LE CACHE UNIFIÃ‰ UFA")
    print("=" * 60)
    print()
    
    combined = []
    seen_keys = set()
    stats = {
        "total_read": 0,
        "duplicates": 0,
        "migrated": 0
    }
    
    # Lire les anciens fichiers
    for f in OLD_FILES:
        if not f.exists():
            print(f"â­ï¸  {f.name} n'existe pas, ignorÃ©")
            continue
        
        print(f"ğŸ“– Lecture de {f.name}...")
        
        if f.suffix == ".json":
            # Fichier JSON classique
            data = read_json(f)
            if data is None:
                continue
            
            # Si c'est un dict, essayer d'extraire les entrÃ©es
            if isinstance(data, dict):
                entries = list(data.values()) if data else []
            elif isinstance(data, list):
                entries = data
            else:
                print(f"   âš ï¸  Format non reconnu, ignorÃ©")
                continue
        else:
            # Fichier JSONL
            entries = read_jsonl(f)
        
        stats["total_read"] += len(entries)
        print(f"   âœ… {len(entries)} entrÃ©es trouvÃ©es")
        
        for e in entries:
            # Normaliser l'entrÃ©e
            normalized = normalize_entry(e, f.stem)
            
            # GÃ©nÃ©rer clÃ© pour dÃ©tection doublons
            key = generate_key(normalized)
            
            if key in seen_keys:
                stats["duplicates"] += 1
                continue
            
            combined.append(normalized)
            seen_keys.add(key)
            stats["migrated"] += 1
    
    # Ajouter ceux dÃ©jÃ  dans le nouveau cache (Ã©viter Ã©crasement)
    if TARGET.exists():
        print(f"ğŸ“– Lecture du cache actuel {TARGET.name}...")
        current = read_jsonl(TARGET)
        stats["total_read"] += len(current)
        print(f"   âœ… {len(current)} entrÃ©es existantes")
        
        for e in current:
            key = generate_key(e)
            if key not in seen_keys:
                combined.append(e)
                seen_keys.add(key)
                stats["migrated"] += 1
            else:
                stats["duplicates"] += 1
    
    # Ã‰crire le fichier final
    write_jsonl(TARGET, combined)
    
    print()
    print("=" * 60)
    print("âœ… MIGRATION TERMINÃ‰E")
    print("=" * 60)
    print(f"ğŸ“Š Statistiques :")
    print(f"   â€¢ EntrÃ©es lues au total : {stats['total_read']}")
    print(f"   â€¢ Doublons dÃ©tectÃ©s : {stats['duplicates']}")
    print(f"   â€¢ EntrÃ©es migrÃ©es : {stats['migrated']}")
    print(f"   â€¢ Fichier de sortie : {TARGET}")
    print()
    
    # Afficher un aperÃ§u
    if combined:
        print("ğŸ“‹ AperÃ§u des 5 premiÃ¨res analyses :")
        for i, entry in enumerate(combined[:5], 1):
            home = entry.get("home_team", "?")
            away = entry.get("away_team", "?")
            league = entry.get("league", "?")
            score = entry.get("prediction", {}).get("most_probable", "?")
            print(f"   {i}. {home} vs {away} ({league}) â†’ {score}")

def generate_report(combined, stats):
    """GÃ©nÃ¨re un rapport statistique dÃ©taillÃ© de la migration"""
    
    # Statistiques par ligue
    leagues = Counter([e.get("league", "Unknown") for e in combined])
    
    # Statistiques par source
    sources = Counter([e.get("source", "unknown") for e in combined])
    
    # CrÃ©er le rapport
    report = []
    report.append("=" * 80)
    report.append("ğŸ“Š RAPPORT DE MIGRATION - UNIFIED ANALYZER")
    report.append("=" * 80)
    report.append("")
    report.append(f"ğŸ“… Date : {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")
    report.append("")
    report.append("ğŸ“ˆ RÃ‰SULTATS GLOBAUX:")
    report.append(f"   â€¢ Total analyses : {len(combined)}")
    report.append(f"   â€¢ EntrÃ©es lues : {stats['total_read']}")
    report.append(f"   â€¢ Doublons Ã©vitÃ©s : {stats['duplicates']}")
    report.append(f"   â€¢ Nouvelles entrÃ©es : {stats['migrated']}")
    report.append("")
    report.append("ğŸ† RÃ‰PARTITION PAR LIGUE:")
    for league, count in leagues.most_common():
        report.append(f"   â€¢ {league}: {count} analyses")
    report.append("")
    report.append("ğŸ“ RÃ‰PARTITION PAR SOURCE:")
    for source, count in sources.most_common():
        report.append(f"   â€¢ {source}: {count} analyses")
    report.append("")
    report.append(f"ğŸ’¾ Fichier final : {TARGET}")
    report.append("=" * 80)
    
    report_text = "\n".join(report)
    
    # Ã‰crire dans le fichier de log
    REPORT_LOG.parent.mkdir(parents=True, exist_ok=True)
    with REPORT_LOG.open("a", encoding="utf-8") as f:
        f.write(report_text + "\n\n")
    
    # Afficher dans la console
    print(report_text)
    
    # Retourner le rÃ©sumÃ© court pour les logs
    summary = f"âœ… Migration rÃ©ussie : {len(combined)} analyses totales ({stats['migrated']} nouvelles)"
    league_summary = " | ".join([f"{league}: {count}" for league, count in leagues.most_common(6)])
    return f"{summary}\n   â†’ {league_summary}\nğŸ“… DerniÃ¨re mise Ã  jour : {datetime.utcnow().strftime('%Y-%m-%d %H:%M')}"

def migrate_and_report():
    """
    Fonction principale exportable pour le scheduler.
    Effectue la migration et retourne un rÃ©sumÃ©.
    """
    print("=" * 60)
    print("ğŸ”„ MIGRATION DES ANALYSES VERS LE CACHE UNIFIÃ‰ UFA")
    print("=" * 60)
    print()
    
    combined = []
    seen_keys = set()
    stats = {
        "total_read": 0,
        "duplicates": 0,
        "migrated": 0
    }
    
    # Lire les anciens fichiers
    for f in OLD_FILES:
        if not f.exists():
            print(f"â­ï¸  {f.name} n'existe pas, ignorÃ©")
            continue
        
        print(f"ğŸ“– Lecture de {f.name}...")
        
        if f.suffix == ".json":
            # Fichier JSON classique
            data = read_json(f)
            if data is None:
                continue
            
            # Si c'est un dict, essayer d'extraire les entrÃ©es
            if isinstance(data, dict):
                entries = list(data.values()) if data else []
            elif isinstance(data, list):
                entries = data
            else:
                print(f"   âš ï¸  Format non reconnu, ignorÃ©")
                continue
        else:
            # Fichier JSONL
            entries = read_jsonl(f)
        
        stats["total_read"] += len(entries)
        print(f"   âœ… {len(entries)} entrÃ©es trouvÃ©es")
        
        for e in entries:
            # Normaliser l'entrÃ©e
            normalized = normalize_entry(e, f.stem)
            
            # GÃ©nÃ©rer clÃ© pour dÃ©tection doublons
            key = generate_key(normalized)
            
            if key in seen_keys:
                stats["duplicates"] += 1
                continue
            
            combined.append(normalized)
            seen_keys.add(key)
            stats["migrated"] += 1
    
    # Ajouter ceux dÃ©jÃ  dans le nouveau cache (Ã©viter Ã©crasement)
    if TARGET.exists():
        print(f"ğŸ“– Lecture du cache actuel {TARGET.name}...")
        current = read_jsonl(TARGET)
        stats["total_read"] += len(current)
        print(f"   âœ… {len(current)} entrÃ©es existantes")
        
        for e in current:
            key = generate_key(e)
            if key not in seen_keys:
                combined.append(e)
                seen_keys.add(key)
                stats["migrated"] += 1
            else:
                stats["duplicates"] += 1
    
    # Ã‰crire le fichier final
    write_jsonl(TARGET, combined)
    
    # GÃ©nÃ©rer et afficher le rapport
    summary = generate_report(combined, stats)
    
    return summary

if __name__ == "__main__":
    try:
        migrate_and_report()
    except Exception as e:
        print(f"âŒ Erreur fatale : {e}")
        import traceback
        traceback.print_exc()
